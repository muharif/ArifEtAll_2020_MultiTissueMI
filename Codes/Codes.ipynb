{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important!\n",
    "\n",
    "To be able to follow the script, please download the data from the GSEXXXXX. Please make sure that the name and filepath in the code is correct as per your downloaded files and file locations. If you encounter any problem, please feel free to contact muhammad.arif@scilifelab.se\n",
    "\n",
    "The codes include all of the analysis steps for differential expression and functional analysis and also network generation and analysis. Codes and data for multi-tissue modelling will be placed under \"WBM\" folder\n",
    "\n",
    "This code was re-tested on Python 3.7 with all of the required modules installed via conda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differential Expression and Functional Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import rpy2,os,re\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib_venn as venn\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42\n",
    "from sklearn.decomposition import PCA as sklearnPCA\n",
    "import seaborn as sns\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "import pickle\n",
    "#rpy2\n",
    "import rpy2.robjects as robjects\n",
    "import rpy2.robjects.numpy2ri\n",
    "rpy2.robjects.numpy2ri.activate()\n",
    "from rpy2.robjects import pandas2ri\n",
    "pandas2ri.activate()\n",
    "import warnings\n",
    "from rpy2.rinterface import RRuntimeWarning\n",
    "warnings.filterwarnings(\"ignore\", category=RRuntimeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DESeq_Piano:\n",
    "    def __init__(self,respath='.'):\n",
    "        if respath=='.':\n",
    "            print('Result Path is Not Defined, any result will be written in this directory')\n",
    "            self.respath=respath\n",
    "        elif os.path.isdir(respath):\n",
    "            print('Result Path Exists, please be careful of overwriting')\n",
    "            self.respath=respath\n",
    "        else:\n",
    "            print('Result Path Does Not Exists, creating new directory')\n",
    "            os.mkdir(respath)\n",
    "            self.respath=respath\n",
    "        self.__Init()\n",
    "\n",
    "            \n",
    "    def __Init(self):\n",
    "\n",
    "        \n",
    "        robjects.r('''\n",
    "            library('DESeq2')\n",
    "            library('piano')\n",
    "            library('Biobase')\n",
    "            library('snow')\n",
    "            library('RColorBrewer')\n",
    "            library('gplots')\n",
    "            library('snowfall')\n",
    "        ''')\n",
    "        self.GSC_KEGG=None\n",
    "        self.GSC_TF=None\n",
    "        self.GSC_RM=None\n",
    "        self.GSC_GO=None\n",
    "        self.GSC_extraGSC=None\n",
    "        self.KEGG=None\n",
    "        self.TF=None\n",
    "        self.RM=None\n",
    "        self.GO=None\n",
    "        self.extraGSC=None\n",
    "        self.dds=None\n",
    "        self.deseq_res=None\n",
    "        self.conds=None\n",
    "        self.count=None\n",
    "        self.tpm=None\n",
    "\n",
    "    def __check_dir(self,extrapath):\n",
    "        if os.path.isdir('%s/%s/' % (self.respath,extrapath)):\n",
    "            print('Path %s/%s/ exist, writing in the folder' % (self.respath,extrapath))\n",
    "        else:\n",
    "            print('Path %s/%s/ does not exist, making the folder to write the results' % (self.respath,extrapath))\n",
    "            os.mkdir('%s/%s/' % (self.respath,extrapath))\n",
    "            \n",
    "    def DEseq(self,count_df=None,conds_series=None):\n",
    "        if count_df is not None:\n",
    "            self.count=count_df\n",
    "        if conds_series is not None:\n",
    "            self.conds=conds_series\n",
    "        self.gene_names=count_df.index\n",
    "        robjects.globalenv['conds']=np.array(self.conds)\n",
    "        robjects.globalenv['subject']=np.array(self.conds.index)\n",
    "        robjects.globalenv['count']=self.count.astype(int).values\n",
    "        robjects.r('''\n",
    "            library('DESeq2')\n",
    "            conds=as.factor(conds)\n",
    "            coldata <- data.frame(row.names=subject,conds)\n",
    "            dds <- DESeqDataSetFromMatrix(countData=as.matrix(count),colData=coldata,design=~conds)\n",
    "            dds <- DESeq(dds)\n",
    "            print(resultsNames(dds))\n",
    "            #baseMeanPerLvl <- sapply( levels(conds), function(lvl) rowMeans( counts(dds,normalized=TRUE)[,conds == lvl] ) )\n",
    "            #conds2=colnames(baseMeanPerLvl)\n",
    "            #gene_names=rownames(baseMeanPerLvl)\n",
    "\n",
    "        ''')\n",
    "        self.dds=robjects.globalenv['dds']\n",
    "    \n",
    "    def DEseq_Compare(self, cond1, cond2,save=True):\n",
    "        if self.dds is None:\n",
    "            raise ValueError('No DESeq data found, run the DEseq function')\n",
    "        else:\n",
    "            print('Comparing %s and %s' % (cond1,cond2))\n",
    "            robjects.globalenv['dds']=self.dds\n",
    "            robjects.globalenv['cond1']=cond1\n",
    "            robjects.globalenv['cond2']=cond2\n",
    "            robjects.globalenv['gene_names']=np.array(self.gene_names)\n",
    "            robjects.r('''\n",
    "                res=results(dds,contrast=c('conds',cond1,cond2))\n",
    "                res=data.frame(res)\n",
    "                res$rownames=gene_names\n",
    "            ''')\n",
    "            res=pandas2ri.rpy2py_dataframe(robjects.globalenv['res']).set_index('rownames')\n",
    "            self.deseq_df=res\n",
    "            if save:\n",
    "                self.__check_dir('deseq')\n",
    "                res.to_csv('%s/deseq/deseq_%s_%s.txt' % (self.respath,cond1,cond2),sep='\\t')\n",
    "        \n",
    "    def loadGSC(self,KEGG='',TF='',RM='',GO='',extraGSC=''):\n",
    "        if KEGG != '':\n",
    "            name='KEGG'\n",
    "            self.__check_dir(name)\n",
    "            self.GSC_KEGG=self.__loadGSC_execute(KEGG)\n",
    "        if TF != '':\n",
    "            name='TF'\n",
    "            self.__check_dir(name)\n",
    "            self.GSC_TF=self.__loadGSC_execute(TF)\n",
    "        if RM != '':\n",
    "            name='RM'\n",
    "            self.__check_dir(name)\n",
    "            self.GSC_RM=self.__loadGSC_execute(RM)\n",
    "        if GO != '':\n",
    "            name='GO'\n",
    "            self.__check_dir(name)\n",
    "            self.GSC_GO=self.__loadGSC_execute(GO)\n",
    "        if extraGSC != '':\n",
    "            name='extraGSC'\n",
    "            self.__check_dir(name)\n",
    "            self.GSC_extraGSC=self.__loadGSC_execute(extraGSC)\n",
    "\n",
    "    \n",
    "    def __loadGSC_execute(self,GSC):\n",
    "        robjects.globalenv['GSC']=GSC \n",
    "        robjects.r('''\n",
    "            y=loadGSC(GSC)\n",
    "        ''')\n",
    "        return robjects.globalenv['y']\n",
    "    \n",
    "    def __PIANO_execute(self,cond1,cond2,GSCtype='',save=True):\n",
    "        robjects.globalenv['deseq_file']=pandas2ri.py2rpy_pandasdataframe(self.deseq_df)\n",
    "        if GSCtype == 'KEGG':\n",
    "            robjects.globalenv['y']=self.GSC_KEGG\n",
    "            deseq_df=self.deseq_df\n",
    "            deseq_df.index=[i.upper() for i in deseq_df.index]\n",
    "            robjects.globalenv['deseq_file']=pandas2ri.py2rpy_pandasdataframe(deseq_df)\n",
    "        elif GSCtype == 'TF':\n",
    "            robjects.globalenv['y']=self.GSC_TF\n",
    "        elif GSCtype == 'RM':\n",
    "            robjects.globalenv['y']=self.GSC_RM\n",
    "        elif GSCtype == 'GO':\n",
    "            robjects.globalenv['y']=self.GSC_GO\n",
    "            deseq_df=self.deseq_df\n",
    "            deseq_df.index=[i.upper() for i in deseq_df.index]\n",
    "            robjects.globalenv['deseq_file']=pandas2ri.py2rpy_pandasdataframe(deseq_df)\n",
    "        else:\n",
    "            robjects.globalenv['y']=self.GSC_extraGSC\n",
    "        robjects.r('''\n",
    "            DESeq_file=deseq_file\n",
    "            DESeq_file=DESeq_file[ ,c('log2FoldChange','pvalue')]\n",
    "            logFC=as.matrix(DESeq_file[,1])\n",
    "            pval=as.matrix(DESeq_file[,2])\n",
    "            rownames(logFC)=(rownames(DESeq_file))\n",
    "            rownames(pval)=(rownames(DESeq_file))\n",
    "            logFC[is.na(logFC)] <- 0\n",
    "            pval[is.na(pval)] <- 1\n",
    "            gsaRes <- runGSA(pval,logFC,gsc=y, geneSetStat=\"reporter\", signifMethod=\"nullDist\", nPerm=1000,ncpus=8)\n",
    "            res_piano=GSAsummaryTable(gsaRes)\n",
    "            res_piano$rownames=rownames(res_piano)\n",
    "        ''')\n",
    "        res=pandas2ri.rpy2py_dataframe(robjects.globalenv['res_piano']).set_index('rownames').set_index('Name').iloc[0:,0:]#.groupby('Name').sum()\n",
    "        if save:\n",
    "            res.to_csv('%s/%s/piano_%s_%s.txt' % (self.respath,GSCtype,cond1,cond2),sep='\\t')\n",
    "        return res\n",
    "\n",
    "    def PIANO_heatmap(self,cond1,cond2,GSCtype,save=True):\n",
    "        if GSCtype == 'KEGG':\n",
    "            deseq=self.KEGG\n",
    "        elif GSCtype == 'TF':\n",
    "            deseq=self.TF\n",
    "        elif GSCtype == 'RM':\n",
    "            deseq=self.RM\n",
    "        elif GSCtype == 'GO':\n",
    "            deseq=self.GO\n",
    "        else:\n",
    "            deseq=self.extraGSC\n",
    "        deseq=deseq[['p adj (dist.dir.up)','p adj (dist.dir.dn)']]\n",
    "        thr=0.05\n",
    "        dn=deseq[(deseq.loc[:,deseq.columns.str.contains('dn')]<thr)].drop_duplicates()\n",
    "        dn=dn.loc[:,dn.columns.str.contains('dn')]\n",
    "        up=deseq[(deseq.loc[:,deseq.columns.str.contains('up')]<thr)].drop_duplicates()\n",
    "        up=up.loc[:,up.columns.str.contains('up')]\n",
    "        temp_dn=np.log10(dn.loc[:,dn.columns.str.contains('adj')][(dn.loc[:,dn.columns.str.contains('adj')]<0.05).sum(1)>0])\n",
    "        temp_dn.index=[i.split('&')[0] if len(i)>100 else i for i in temp_dn.index]\n",
    "        temp_up=-np.log10(up.loc[:,up.columns.str.contains('adj')][(up.loc[:,up.columns.str.contains('adj')]<0.05).sum(1)>0])\n",
    "        temp_up.index=[i.split('&')[0] if len(i)>100 else i for i in temp_up.index]\n",
    "        name=list(set(temp_up.index).intersection(set(temp_dn.index)))\n",
    "        temp_df=pd.concat([temp_up.loc[name],temp_dn.loc[name]])\n",
    "        temp_up=temp_up.loc[list(set(temp_up.index).difference(set(name)))]\n",
    "        temp_dn=(temp_dn.loc[list(set(temp_dn.index).difference(set(name)))])\n",
    "        temp_df=temp_df.groupby(temp_df.index).max()\n",
    "        temp=pd.concat([temp_dn,temp_up,temp_df]).T\n",
    "        temp=pd.DataFrame(temp.sum(),columns=['%s vs %s' % (cond1,cond2)])\n",
    "        temp=temp.replace(np.inf,temp.replace([np.inf, -np.inf],np.nan).max().max()*1.1)\n",
    "        temp=temp.replace(-np.inf,temp.replace([np.inf, -np.inf],np.nan).min().min()*1.1)\n",
    "        temp=temp.replace(-0.0,0)\n",
    "        temp=temp.loc[:,sorted(temp.columns)]\n",
    "        min1=temp.min().min()\n",
    "        max1=temp.max().max()\n",
    "        cmap=matplotlib.colors.LinearSegmentedColormap.from_list(\"\", [\"#0000a5\",'#0000d8',\"#FFFAF0\",'#d80000',\"#a50000\"])\n",
    "        temp=temp.replace(np.inf,temp.replace([np.inf, -np.inf],np.nan).max().max()*1.1)\n",
    "        temp=temp.replace(-np.inf,temp.replace([np.inf, -np.inf],np.nan).min().min()*1.1)\n",
    "        temp=temp[(temp.abs()>-np.log10(thr)).sum(1)>0].fillna(0)\n",
    "        temp2=temp.reindex(temp[temp.columns[0]].abs().sort_values(ascending=False).index).copy()\n",
    "        if temp.shape[0] > 100:\n",
    "            temp=temp.reindex(temp[temp.columns[0]].abs().sort_values(ascending=False).index)\n",
    "            temp_d=temp[temp[temp.columns[0]] < 0]\n",
    "            temp_u=temp[temp[temp.columns[0]] > 0]\n",
    "            if temp_d.shape[0]<=0:\n",
    "                temp_u=temp_u.iloc[0:(100-temp_d.shape[0])]\n",
    "            elif temp_u.shape[0]<=0:\n",
    "                temp_d=temp_d.iloc[0:(100-temp_u.shape[0])]\n",
    "            else:\n",
    "                temp_d=temp_d.iloc[0:50]\n",
    "                temp_u=temp_u.iloc[0:50]\n",
    "            temp=pd.concat([temp_d,temp_u])\n",
    "            g=sns.clustermap(temp,figsize=(2.5, 10),col_cluster=False,row_cluster=True,linewidths=0.1,cmap=cmap,center=0.0,cbar_kws={'label': '-Log10(P-Adjusted)'})\n",
    "            g.ax_heatmap.set(yticks=[i-0.5 for i in range(1,temp.shape[0]+1)])\n",
    "            g.ax_heatmap.set_yticklabels(labels=temp.index[list(map(int,g.dendrogram_row.calculate_dendrogram()['ivl']))],size=6)\n",
    "        else:\n",
    "            g=sns.clustermap(temp,figsize=(2.5, 10),col_cluster=False,row_cluster=True,linewidths=0.1,cmap=cmap,center=0.0,cbar_kws={'label': '-Log10(P-Adjusted)'})\n",
    "            g.ax_heatmap.set(yticks=[i-0.5 for i in range(1,temp.shape[0]+1)])\n",
    "            g.ax_heatmap.set_yticklabels(labels=temp.index[list(map(int,g.dendrogram_row.calculate_dendrogram()['ivl']))],size=8)\n",
    "        g.ax_heatmap.xaxis.tick_top()\n",
    "        path=self.respath+'/Figures/'+GSCtype\n",
    "        if save:\n",
    "            g.savefig('%s/Heatmap_%s_%s.pdf' % (path,cond1,cond2))\n",
    "    \n",
    "    def PIANO(self,cond1,cond2,heatmap=True,save=True):\n",
    "        if save:\n",
    "            self.__check_dir('Figures')\n",
    "        if self.deseq_df is None:\n",
    "            raise ValueError('No DEseq comparison found, run DEseq_Compare first or assign deseq result dataframe to deseq_res')\n",
    "        else:\n",
    "            if self.GSC_KEGG is not None:\n",
    "                name='KEGG'\n",
    "                self.KEGG = self.__PIANO_execute(cond1,cond2,GSCtype=name,save=save)\n",
    "                if heatmap == True:\n",
    "                    self.__check_dir('/Figures/%s' % name)\n",
    "                    try:\n",
    "                        self.PIANO_heatmap(cond1,cond2,GSCtype=name,save=save)\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "            if self.GSC_GO is not None:\n",
    "                name='GO'\n",
    "                self.GO = self.__PIANO_execute(cond1,cond2,GSCtype=name,save=save)\n",
    "                if heatmap == True:\n",
    "                    self.__check_dir('/Figures/%s' % name)\n",
    "                    try:\n",
    "                        self.PIANO_heatmap(cond1,cond2,GSCtype=name,save=save)\n",
    "                    except ValueError:\n",
    "                        pass            \n",
    "            if self.GSC_TF is not None:\n",
    "                name='TF'\n",
    "                self.TF = self.__PIANO_execute(cond1,cond2,GSCtype=name,save=save)\n",
    "                if heatmap == True:\n",
    "                    self.__check_dir('/Figures/%s' % name)\n",
    "                    try:\n",
    "                        self.PIANO_heatmap(cond1,cond2,GSCtype=name,save=save)\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "            if self.GSC_RM is not None:\n",
    "                name='RM'\n",
    "                self.RM = self.__PIANO_execute(cond1,cond2,GSCtype=name,save=save)\n",
    "                if heatmap == True:\n",
    "                    self.__check_dir('/Figures/%s' % name)\n",
    "                    try:\n",
    "                        self.PIANO_heatmap(cond1,cond2,GSCtype=name,save=save)\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "            if self.GSC_extraGSC is not None:\n",
    "                name='extraGSC'\n",
    "                self.extraGSC = self.__PIANO_execute(cond1,cond2,GSCtype=name,save=save)\n",
    "                if heatmap == True:\n",
    "                    self.__check_dir('/Figures/%s' % name)\n",
    "                    try:\n",
    "                        self.PIANO_heatmap(cond1,cond2,GSCtype=name,save=save)\n",
    "                    except ValueError:\n",
    "                        pass    \n",
    "                    \n",
    "    def __piano2xlsx(self,name,part='ALL'):\n",
    "        if part != 'ALL':\n",
    "            files=[i for i in sorted(os.listdir('%s/%s' % (self.respath,name))) if part in i]\n",
    "        else:\n",
    "            files=sorted(os.listdir('%s/%s' % (self.respath,name)))\n",
    "        writer = pd.ExcelWriter('%s/%s_%s.xlsx' % (self.respath,name,part), engine='xlsxwriter')\n",
    "        for i in files:\n",
    "            if i.startswith('.'):\n",
    "                continue\n",
    "            deseq1=pd.read_csv('%s/%s/%s' % (self.respath,name,i),index_col='Name',sep='\\t')[['Genes (tot)','p adj (dist.dir.up)']]\n",
    "            deseq1.columns=['# of Genes','P-Adj']\n",
    "            deseq1=deseq1[deseq1['P-Adj']<0.05]\n",
    "            deseq1['Direction']='UP'\n",
    "            deseq2=pd.read_csv('%s/%s/%s' % (self.respath,name,i),index_col='Name',sep='\\t')[['Genes (tot)','p adj (dist.dir.dn)']]\n",
    "            deseq2.columns=['# of Genes','P-Adj']\n",
    "            deseq2=deseq2[deseq2['P-Adj']<0.05]\n",
    "            deseq2['Direction']='DOWN'\n",
    "            deseq1=pd.concat([deseq1,deseq2])\n",
    "            if len(i.replace('piano_','').replace('.txt',''))>31:\n",
    "                print('sheet name too big, trimming to 30 character')\n",
    "                n_temp=i.replace('piano_','').replace('.txt','')[0:31]\n",
    "            else:\n",
    "                n_temp=i.replace('piano_','').replace('.txt','')\n",
    "            deseq1.sort_values('P-Adj').to_excel(writer, sheet_name=n_temp)\n",
    "        writer.save()\n",
    "        \n",
    "    def summarizePiano(self,names=['KEGG','GO','TF','RM','extraGSC'],part='ALL'):\n",
    "        for name in names:\n",
    "            if os.path.isdir(self.respath+name):\n",
    "                self.__piano2xlsx(name,part=part)\n",
    "            else:\n",
    "                continue\n",
    "    \n",
    "    def summarizeDEseq(self,deseq_result='deseq',part='ALL'):\n",
    "        if os.path.isdir(self.respath+deseq_result):\n",
    "            if part != 'ALL':\n",
    "                files=[i for i in sorted(os.listdir(self.respath+'/deseq/')) if part in i]\n",
    "            else:\n",
    "                files=sorted(os.listdir(self.respath+'/deseq/'))\n",
    "            writer = pd.ExcelWriter(self.respath+'/DifferentialExpression_%s.xlsx' % part, engine='xlsxwriter')\n",
    "            fin=0\n",
    "            for i in files:\n",
    "                if i.startswith('.'):\n",
    "                    continue\n",
    "                deseq=pd.read_csv(self.respath+'/deseq/%s' % i,index_col=0,sep='\\t')\n",
    "                deseq['abs']=deseq['log2FoldChange'].abs()\n",
    "                deseq=deseq.sort_values('abs',ascending=False)[['log2FoldChange','pvalue','padj']]\n",
    "\n",
    "                deseq['Direction']=['UP' if i > 0 else 'DOWN' for i in deseq['log2FoldChange']]\n",
    "                deseq.columns=['L2FC','P-VALUE','P-ADJ','DIRECTION']\n",
    "                deseq=deseq.sort_values('P-VALUE')\n",
    "                if len(i.replace('deseq_','').replace('.txt',''))>31:\n",
    "                    print('sheet name too big, trimming to 30 character')\n",
    "                    n_temp=i.replace('deseq_','').replace('.txt','')[0:31]\n",
    "                else:\n",
    "                    n_temp=i.replace('deseq_','').replace('.txt','')\n",
    "                deseq.to_excel(writer, sheet_name=n_temp)\n",
    "            writer.save()\n",
    "        else:\n",
    "            raise ValueError('No DESEq result found')\n",
    "\n",
    "    def save_object(self,filename='DEseq_PIANO.pkl'):\n",
    "        with open('%s/%s' % (self.respath,filename), 'wb') as file:\n",
    "            pickle.dump(self, file)\n",
    "    \n",
    "    def load_object(self,filename='DEseq_PIANO.pkl'):\n",
    "        with open(filename, 'rb') as file:\n",
    "            res=pickle.load(file)\n",
    "        return res\n",
    "    \n",
    "    def PCA(self,tpm='',conds='',annot='',respath='',save=False,part='',not_part=None):\n",
    "        if type(conds) != pd.core.series.Series:\n",
    "            conds = self.conds\n",
    "        if type(tpm) != pd.core.frame.DataFrame:\n",
    "            tpm = self.tpm\n",
    "        filt=sum(list(map(lambda x: (tpm[conds[conds==x].index].mean(1)>1),set(conds))))\n",
    "        tpm=tpm[(filt>0)]\n",
    "        if ((part == '') & (not_part == None)):\n",
    "            part='ALL'\n",
    "        elif part != '':\n",
    "            conds=conds[conds.str.contains(part)]\n",
    "            tpm=tpm[conds.index]\n",
    "        elif not_part != None:\n",
    "            part='NOT %s' % not_part\n",
    "            conds=conds[~conds.str.contains(part)]\n",
    "            tpm=tpm[conds.index]\n",
    "        sklearn_pca = sklearnPCA(n_components=2)\n",
    "        Y_sklearn = sklearn_pca.fit_transform(np.log10(tpm.T+1))\n",
    "        with plt.style.context('seaborn-white'):\n",
    "            fig = plt.figure(figsize=(15,8))\n",
    "            ax = fig.add_subplot(111, aspect='auto')\n",
    "            ax.grid(False)\n",
    "            for lab,m,col in zip(sorted(list(set(conds))),['o','*','v','^']*4,(['k']*4)+(['lightblue']*4)+(['yellow']*4)+(['green']*4)):\n",
    "                x=Y_sklearn[conds==lab, 0]\n",
    "                y=Y_sklearn[conds==lab, 1]\n",
    "                plt.scatter(x,y,c=col,label=lab,marker=m,s=200)\n",
    "        if annot != '':\n",
    "            for i, txt in enumerate(annot):\n",
    "                ax.annotate(txt, (Y_sklearn[i, 0], Y_sklearn[i, 1]))\n",
    "        plt.xlabel('PC1 (%.2f)' % (sklearn_pca.explained_variance_ratio_[0]*100))\n",
    "        plt.ylabel('PC2 (%.2f)' % (sklearn_pca.explained_variance_ratio_[1]*100))\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "        plt.tight_layout()\n",
    "        if save:\n",
    "            self.__check_dir('Figures')\n",
    "            plt.savefig('%s/Figures/PCA_%s.pdf' % (self.respath,part))\n",
    "        loadings=sklearn_pca.components_.T * np.sqrt(sklearn_pca.explained_variance_)\n",
    "        loadings=pd.DataFrame(loadings,columns=['x','y'],index=tpm.index)\n",
    "        return loadings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=pd.read_csv('../Data/count_geneName.txt',sep='\\t',index_col=0)\n",
    "tpm=pd.read_csv('../Data/tpm_geneName.txt',sep='\\t',index_col=0)\n",
    "conds=pd.read_csv('../Data/metadata.txt',sep='\\t',index_col=0)\n",
    "conds['Conditions']=conds['Tissue']+'_'+conds['Treatment']+'_'+conds['Time']\n",
    "conds=conds['Conditions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the class\n",
    "k=DESeq_Piano('../Results/')\n",
    "\n",
    "#Generate PCA\n",
    "loadings=k.PCA(tpm,conds,save=True)\n",
    "\n",
    "#Execute Differential Expression Analysis\n",
    "k.DEseq(count,conds)\n",
    "\n",
    "#Loading gene set collection for functional analysis\n",
    "k.loadGSC(KEGG='../Library/KEGG.gmt' ,GO='../../MainLibrary/Current/GO_BP.gmt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define conditions to compare, example in heart 24h, SHAM vs MI\n",
    "cond1='heart_MI_24h'\n",
    "cond2='heart_SHAM_24h'\n",
    "\n",
    "#Retrieve DEGs specific for the defined comparion\n",
    "k.DEseq_Compare(cond1,cond2,save=True)\n",
    "\n",
    "#Retrieve Functional analysis results specific for the defined comparion\n",
    "k.PIANO(cond1,cond2,heatmap=True,save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine all results to a single excel file for each type of analysis\n",
    "k.summarizeDEseq()\n",
    "k.summarizePiano()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation of Tissue-specific CN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib, os, shutil\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42\n",
    "from scipy.stats import spearmanr\n",
    "from statsmodels.sandbox.stats.multicomp import multipletests\n",
    "from tqdm import tqdm_notebook\n",
    "import leidenalg\n",
    "import igraph as ig\n",
    "import gseapy as gp\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc(tpm,pval_thr=0.05,pos_only=False):\n",
    "    print('Calculating Correlation..')\n",
    "    temp=spearmanr(tpm.T)\n",
    "    corr=pd.DataFrame(temp[0],columns=list(tpm.index),index=list(tpm.index))\n",
    "    pval=pd.DataFrame(temp[1],columns=list(tpm.index),index=list(tpm.index))\n",
    "    print('Filtering the matrix Correlation..')\n",
    "    corr=corr.where(np.triu(np.ones(corr.shape)).astype(np.bool))\n",
    "    pval=pval.where(np.triu(np.ones(pval.shape)).astype(np.bool))\n",
    "    print('Making long table of Correlation..')\n",
    "    corr2=corr.unstack().reset_index(name='weight')\n",
    "    pval2=pval.unstack().reset_index(name='pval')\n",
    "    res=corr2.merge(pval2,on=['level_0','level_1'])\n",
    "    res=res[res['level_0'] != res['level_1']]\n",
    "    res=res.dropna()\n",
    "    print('Adjusting P-val')\n",
    "    res['padj']=multipletests(res['pval'],method='fdr_bh')[1]\n",
    "    res=res[res.padj < pval_thr].reset_index(drop=True)\n",
    "    if pos_only:\n",
    "        res=res[res['weight']>0]\n",
    "    res=res[['level_0','level_1','weight']]\n",
    "    print('Done!!')\n",
    "    return res\n",
    "def filter_tpm(tpm,conds,thr=1):\n",
    "    #making sure that in at least 1 condition we have TPM > threshold (default 1)\n",
    "    return tpm[(pd.concat([tpm.T,conds],1).groupby(conds.name).mean().T>thr).sum(1)>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpm=pd.read_csv('../Data/tpm_geneName.txt',sep='\\t',index_col=0)\n",
    "conds=pd.read_csv('../Data/metadata.txt',sep='\\t',index_col=0)\n",
    "conds=conds['Tissue']\n",
    "tpm=tpm[conds.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the specific tissue, example heart\n",
    "tissue='heart'\n",
    "conds2=conds[conds == tissue]\n",
    "tpm2=tpm[conds2.index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate co-expression network\n",
    "corr=calc(filter_tpm(tpm2,conds2))\n",
    "corr.to_csv('../Results/coexp_%s.txt' % tissue,sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CN Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network_Analysis:\n",
    "    def __init__(self,filename,respath,KEGG='',GO='',topx=0.05,cluster_size=30):\n",
    "        self.network_ori=pd.read_csv(filename,sep='\\t')\n",
    "        self.res_path=respath\n",
    "        if os.path.isdir(self.res_path):\n",
    "            pass\n",
    "        else:\n",
    "            os.mkdir(self.res_path)\n",
    "#         self.min_weight=min_weight\n",
    "        self.cluster_size=cluster_size\n",
    "        self.topx=topx\n",
    "        self.GSC_KEGG=KEGG\n",
    "        self.GSC_GO=GO\n",
    "        self.__net_analysis()\n",
    "        self.__net_analysis_neg()\n",
    "        print('Enriching Clusters...')\n",
    "        self.__cluster_enrichr()\n",
    "        self.__cluster_enrichr_neg()\n",
    "        \n",
    "    def __net_analysis(self):\n",
    "        print('Loading The Network...')\n",
    "#         temp=self.network_ori[self.network_ori['weight']>self.min_weight]\n",
    "        temp=self.network_ori[self.network_ori['weight']>0]\n",
    "        temp=temp[temp['weight']>temp['weight'].quantile(1-self.topx)]\n",
    "        g= ig.Graph.TupleList(zip(temp['level_0'],temp['level_1']))\n",
    "        print('Cluster Analysis...')\n",
    "        optimiser = leidenalg.Optimiser()\n",
    "        clust_calc = leidenalg.ModularityVertexPartition(g)\n",
    "        diff =  optimiser.optimise_partition(clust_calc, n_iterations=-1)\n",
    "        clust=pd.Series(clust_calc.membership,index=g.vs['name'])\n",
    "        thr=self.cluster_size\n",
    "        temp_c=clust.value_counts()[clust.value_counts()>thr].index.tolist()\n",
    "        self.clustering=clust[clust.isin(temp_c)]\n",
    "        self.modularity=clust_calc.modularity\n",
    "        self.network=g\n",
    "    \n",
    "    def __net_analysis_neg(self):\n",
    "        print('Loading The Network...')\n",
    "#         temp=self.network_ori[self.network_ori['weight']<(-1*self.min_weight)]\n",
    "        temp=self.network_ori[self.network_ori['weight']<0]\n",
    "        temp=temp[temp['weight']<temp['weight'].quantile(self.topx)]\n",
    "        g= ig.Graph.TupleList(zip(temp['level_0'],temp['level_1']))\n",
    "        print('Cluster Analysis...')\n",
    "        optimiser = leidenalg.Optimiser()\n",
    "        clust_calc = leidenalg.ModularityVertexPartition(g)\n",
    "        diff =  optimiser.optimise_partition(clust_calc, n_iterations=-1)\n",
    "        clust=pd.Series(clust_calc.membership,index=g.vs['name'])\n",
    "        thr=self.cluster_size\n",
    "        temp_c=clust.value_counts()[clust.value_counts()>thr].index.tolist()\n",
    "        self.clustering_neg=clust[clust.isin(temp_c)]\n",
    "        self.modularity_neg=clust_calc.modularity\n",
    "        self.network_neg=g\n",
    "    \n",
    "    def __enrichr(self,GSC,db_name):\n",
    "        # Offline GSEA with enrichr\n",
    "        final={}\n",
    "        for i in range(self.clustering.value_counts().shape[0]):\n",
    "            if db_name == 'GO':\n",
    "                gene_list=list([i.upper() for i in self.clustering[self.clustering.isin([i])].index])\n",
    "            else:\n",
    "                gene_list=list([i.upper() for i in self.clustering[self.clustering.isin([i])].index])\n",
    "            enr = gp.enrichr(gene_list=gene_list,\n",
    "                             description='test_name',\n",
    "                             gene_sets=GSC,\n",
    "                             background=20000,\n",
    "                             outdir='test/enrichr_kegg2',\n",
    "                             cutoff=0.5,\n",
    "                             verbose=False)\n",
    "            \n",
    "            temp=enr.res2d[enr.res2d['Adjusted P-value']<0.05].sort_values('Adjusted P-value')[['Term','Adjusted P-value']]\n",
    "            temp.columns=['%s' % db_name,'P-Adj (%s)' % db_name]\n",
    "            if db_name == 'GO':\n",
    "                temp['GOID']=[s[s.find(\"(\")+1:s.find(\")\")] for s in temp['GO']]\n",
    "                temp=temp[['GO','GOID','P-Adj (GO)']]\n",
    "            final[i]=temp.reset_index(drop=True)\n",
    "        return final\n",
    "    \n",
    "    def __cluster_enrichr(self):\n",
    "        if (self.GSC_KEGG != '') | (self.GSC_GO != ''):\n",
    "            writer = pd.ExcelWriter('%s/Coexp_Cluste_Enrichr.xlsx' % self.res_path, engine='xlsxwriter')\n",
    "            if self.GSC_KEGG != '':\n",
    "                self.KEGG=self.__enrichr(self.GSC_KEGG,'KEGG')\n",
    "                pd.concat(self.KEGG.values(), axis=1, keys=self.KEGG.keys()).to_excel(writer, sheet_name='KEGG')\n",
    "\n",
    "            if self.GSC_GO != '':\n",
    "                self.GO=self.__enrichr(self.GSC_GO,'GO')\n",
    "                pd.concat(self.GO.values(), axis=1, keys=self.GO.keys()).to_excel(writer, sheet_name='GO')\n",
    "\n",
    "            if (self.GSC_KEGG != '') & (self.GSC_GO != ''):\n",
    "                self.Both={i:pd.concat([self.GO[i],self.KEGG[i]],1) for i in range(self.clustering.value_counts().shape[0])}\n",
    "                pd.concat(self.Both.values(), axis=1, keys=self.Both.keys()).to_excel(writer, sheet_name='Both')\n",
    "            writer.save()\n",
    "            shutil.rmtree('test')\n",
    "    \n",
    "    def __enrichr_neg(self,GSC,db_name):\n",
    "        # Offline GSEA with enrichr\n",
    "        final={}\n",
    "        for i in range(self.clustering_neg.value_counts().shape[0]):\n",
    "            if db_name == 'GO':\n",
    "                gene_list=list([i.upper() for i in self.clustering_neg[self.clustering_neg.isin([i])].index])\n",
    "            else:\n",
    "                gene_list=list([i.upper() for i in self.clustering_neg[self.clustering_neg.isin([i])].index])\n",
    "            enr = gp.enrichr(gene_list=gene_list,\n",
    "                             description='test_name',\n",
    "                             gene_sets=GSC,\n",
    "                             background=20000,\n",
    "                             outdir='test/enrichr_kegg2',\n",
    "                             cutoff=0.5,\n",
    "                             verbose=False)\n",
    "            temp=enr.res2d[enr.res2d['Adjusted P-value']<0.05].sort_values('Adjusted P-value')[['Term','Adjusted P-value']]\n",
    "            temp.columns=['%s' % db_name,'P-Adj (%s)' % db_name]\n",
    "            if db_name == 'GO':\n",
    "                temp['GOID']=[s[s.find(\"(\")+1:s.find(\")\")] for s in temp['GO']]\n",
    "                temp=temp[['GO','GOID','P-Adj (GO)']]\n",
    "            final[i]=temp.reset_index(drop=True)\n",
    "        return final\n",
    "    \n",
    "    def __cluster_enrichr_neg(self):\n",
    "        if (self.GSC_KEGG != '') | (self.GSC_GO != ''):\n",
    "            writer = pd.ExcelWriter('%s/Coexp_Cluste_Enrichr_neg.xlsx' % self.res_path, engine='xlsxwriter')\n",
    "            if self.GSC_KEGG != '':\n",
    "                self.KEGG_neg=self.__enrichr_neg(self.GSC_KEGG,'KEGG')\n",
    "                pd.concat(self.KEGG_neg.values(), axis=1, keys=self.KEGG_neg.keys()).to_excel(writer, sheet_name='KEGG')\n",
    "\n",
    "            if self.GSC_GO != '':\n",
    "                self.GO_neg=self.__enrichr_neg(self.GSC_GO,'GO')\n",
    "                pd.concat(self.GO_neg.values(), axis=1, keys=self.GO_neg.keys()).to_excel(writer, sheet_name='GO')\n",
    "\n",
    "            if (self.GSC_KEGG != '') & (self.GSC_GO != ''):\n",
    "                self.Both_neg={i:pd.concat([self.GO_neg[i],self.KEGG_neg[i]],1) for i in range(self.clustering_neg.value_counts().shape[0])}\n",
    "                pd.concat(self.Both_neg.values(), axis=1, keys=self.Both_neg.keys()).to_excel(writer, sheet_name='Both')\n",
    "            writer.save()\n",
    "            shutil.rmtree('test')\n",
    "    \n",
    "    def save_network(self):\n",
    "        pickle_out = open('%s/network_object.pkl' % self.res_path,\"wb\")\n",
    "        pickle.dump(self, pickle_out)\n",
    "        pickle_out.close()\n",
    "        \n",
    "    def export_cluster_cytoscape(self):\n",
    "        temp=self.network_ori.merge(pd.DataFrame(self.clustering).reset_index(),how='left',left_on='level_0',right_on='index').merge(pd.DataFrame(self.clustering).reset_index(),how='left',left_on='level_1',right_on='index').dropna()\n",
    "        temp2=temp[['0_x','0_y']]\n",
    "        temp2['count']=1\n",
    "        temp2=temp2.groupby(['0_x','0_y']).count().reset_index().pivot_table(index='0_x',columns='0_y',values='count').fillna(0)\n",
    "        temp2=pd.DataFrame([(i,j,(temp2.loc[i,j]+temp2.loc[j,i]),(temp2.loc[i,j]+temp2.loc[j,i])/(self.clustering.value_counts()[i]+self.clustering.value_counts()[j])) for i in temp2.index for j in temp2.columns if (i < j)],columns=['Source','Target','Weight1','Weight2'])\n",
    "        temp2.astype(int).to_csv('%s/edge_pos_cluster.txt' % self.res_path,sep='\\t')\n",
    "        pd.DataFrame(self.clustering.value_counts(),columns=['Size']).to_csv('%s/node_pos_cluster.txt' % self.res_path,sep='\\t')\n",
    "        self.clustering.to_csv('%s/clust_pos_cluster.txt' % self.res_path,sep='\\t')\n",
    "        temp=self.network_ori.merge(pd.DataFrame(self.clustering_neg).reset_index(),how='left',left_on='level_0',right_on='index').merge(pd.DataFrame(self.clustering_neg).reset_index(),how='left',left_on='level_1',right_on='index').dropna()\n",
    "        temp2=temp[['0_x','0_y']]\n",
    "        temp2['count']=1\n",
    "        temp2=temp2.groupby(['0_x','0_y']).count().reset_index().pivot_table(index='0_x',columns='0_y',values='count').fillna(0)\n",
    "        temp2=pd.DataFrame([(i,j,(temp2.loc[i,j]+temp2.loc[j,i]),(temp2.loc[i,j]+temp2.loc[j,i])/(self.clustering_neg.value_counts()[i]+self.clustering_neg.value_counts()[j])) for i in temp2.index for j in temp2.columns if (i < j)],columns=['Source','Target','Weight1','Weight2'])\n",
    "        temp2.astype(int).to_csv('%s/edge_neg_cluster.txt' % self.res_path,sep='\\t')\n",
    "        pd.DataFrame(self.clustering_neg.value_counts(),columns=['Size']).to_csv('%s/node_neg_cluster.txt' % self.res_path,sep='\\t')\n",
    "        self.clustering_neg.to_csv('%s/clust_neg_cluster.txt' % self.res_path,sep='\\t')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tissue='heart'\n",
    "\n",
    "#Self-Explanatory\n",
    "k=Network_Analysis(filename='../Results/coexp_%s.txt' % tissue,respath='../Results/Coexp_%s/' % tissue,KEGG='../Library/KEGG.gmt',GO='../Library/GO_BP.gmt',topx=topx)\n",
    "k.export_cluster_cytoscape()\n",
    "k.save_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
